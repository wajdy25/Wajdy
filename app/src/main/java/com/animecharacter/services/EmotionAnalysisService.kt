package com.animecharacter.services

import android.content.Context
import android.media.AudioFormat
import android.media.AudioRecord
import android.media.MediaRecorder
import android.util.Log
import com.google.cloud.speech.v1.*
import com.google.cloud.language.v1.*
import kotlinx.coroutines.*
import org.json.JSONObject
import java.io.ByteArrayOutputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder

/**
 * Ø®Ø¯Ù…Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø§Ù„ØµÙˆØª ÙˆØ§Ù„Ù†Øµ
 * ØªØ¯Ø¹Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙˆØ§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ù„Ù„Ù…Ø´Ø§Ø¹Ø±
 */
class EmotionAnalysisService(private val context: Context) {
    
    companion object {
        private const val TAG = "EmotionAnalysisService"
        private const val SAMPLE_RATE = 16000
        private const val CHANNEL_CONFIG = AudioFormat.CHANNEL_IN_MONO
        private const val AUDIO_FORMAT = AudioFormat.ENCODING_PCM_16BIT
    }
    
    // Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
    enum class Emotion(val arabicName: String, val intensity: Float) {
        HAPPY("Ø³Ø¹ÙŠØ¯", 0.8f),
        SAD("Ø­Ø²ÙŠÙ†", 0.7f),
        ANGRY("ØºØ§Ø¶Ø¨", 0.9f),
        SURPRISED("Ù…ØªÙØ§Ø¬Ø¦", 0.6f),
        FEAR("Ø®Ø§Ø¦Ù", 0.8f),
        DISGUST("Ù…Ø´Ù…Ø¦Ø²", 0.7f),
        NEUTRAL("Ù…Ø­Ø§ÙŠØ¯", 0.3f),
        EXCITED("Ù…ØªØ­Ù…Ø³", 0.9f),
        CALM("Ù‡Ø§Ø¯Ø¦", 0.4f),
        CONFUSED("Ù…Ø­ØªØ§Ø±", 0.5f)
    }
    
    // Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
    data class EmotionResult(
        val primaryEmotion: Emotion,
        val confidence: Float,
        val emotionScores: Map<Emotion, Float>,
        val arousal: Float, // Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥Ø«Ø§Ø±Ø©
        val valence: Float, // Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©/Ø§Ù„Ø³Ù„Ø¨ÙŠØ©
        val analysisMethod: String
    )
    
    private var audioRecord: AudioRecord? = null
    private var isRecording = false
    private val coroutineScope = CoroutineScope(Dispatchers.IO + SupervisorJob())
    
    /**
     * ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø§Ù„Ù†Øµ
     */
    suspend fun analyzeTextEmotion(text: String): EmotionResult = withContext(Dispatchers.IO) {
        try {
            // ØªØ­Ù„ÙŠÙ„ Ù…Ø­Ù„ÙŠ Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Øµ
            val localResult = analyzeTextLocally(text)
            
            // Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            try {
                val cloudResult = analyzeTextWithCloud(text)
                return@withContext cloudResult ?: localResult
            } catch (e: Exception) {
                Log.w(TAG, "Cloud analysis failed, using local result", e)
                return@withContext localResult
            }
        } catch (e: Exception) {
            Log.e(TAG, "Text emotion analysis failed", e)
            return@withContext EmotionResult(
                Emotion.NEUTRAL, 0.5f, mapOf(Emotion.NEUTRAL to 0.5f),
                0.5f, 0.5f, "fallback"
            )
        }
    }
    
    /**
     * ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø§Ù„ØµÙˆØª
     */
    suspend fun analyzeAudioEmotion(audioData: ByteArray): EmotionResult = withContext(Dispatchers.IO) {
        try {
            // Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
            val audioFeatures = extractAudioFeatures(audioData)
            
            // ØªØ­Ù„ÙŠÙ„ Ù…Ø­Ù„ÙŠ Ù„Ù„ØµÙˆØª
            val localResult = analyzeAudioLocally(audioFeatures)
            
            // Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ
            try {
                val cloudResult = analyzeAudioWithCloud(audioData)
                return@withContext cloudResult ?: localResult
            } catch (e: Exception) {
                Log.w(TAG, "Cloud audio analysis failed, using local result", e)
                return@withContext localResult
            }
        } catch (e: Exception) {
            Log.e(TAG, "Audio emotion analysis failed", e)
            return@withContext EmotionResult(
                Emotion.NEUTRAL, 0.5f, mapOf(Emotion.NEUTRAL to 0.5f),
                0.5f, 0.5f, "fallback"
            )
        }
    }
    
    /**
     * Ø¨Ø¯Ø¡ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
     */
    fun startRealTimeEmotionAnalysis(callback: (EmotionResult) -> Unit) {
        if (isRecording) return
        
        try {
            val bufferSize = AudioRecord.getMinBufferSize(SAMPLE_RATE, CHANNEL_CONFIG, AUDIO_FORMAT)
            audioRecord = AudioRecord(
                MediaRecorder.AudioSource.MIC,
                SAMPLE_RATE,
                CHANNEL_CONFIG,
                AUDIO_FORMAT,
                bufferSize
            )
            
            audioRecord?.startRecording()
            isRecording = true
            
            coroutineScope.launch {
                val buffer = ByteArray(bufferSize)
                val audioBuffer = ByteArrayOutputStream()
                
                while (isRecording) {
                    val bytesRead = audioRecord?.read(buffer, 0, buffer.size) ?: 0
                    if (bytesRead > 0) {
                        audioBuffer.write(buffer, 0, bytesRead)
                        
                        // ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ø«Ø§Ù†ÙŠØªÙŠÙ†
                        if (audioBuffer.size() >= SAMPLE_RATE * 2 * 2) { // 2 seconds of 16-bit audio
                            val audioData = audioBuffer.toByteArray()
                            audioBuffer.reset()
                            
                            val emotion = analyzeAudioEmotion(audioData)
                            withContext(Dispatchers.Main) {
                                callback(emotion)
                            }
                        }
                    }
                    delay(100) // ØªØ­Ø¯ÙŠØ« ÙƒÙ„ 100ms
                }
            }
        } catch (e: Exception) {
            Log.e(TAG, "Failed to start real-time emotion analysis", e)
        }
    }
    
    /**
     * Ø¥ÙŠÙ‚Ø§Ù ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
     */
    fun stopRealTimeEmotionAnalysis() {
        isRecording = false
        audioRecord?.stop()
        audioRecord?.release()
        audioRecord = null
    }
    
    /**
     * ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø©
     */
    private fun analyzeTextLocally(text: String): EmotionResult {
        val lowerText = text.lowercase()
        
        // ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        val emotionKeywords = mapOf(
            Emotion.HAPPY to listOf("Ø³Ø¹ÙŠØ¯", "ÙØ±Ø­", "Ù…Ø¨Ø³ÙˆØ·", "Ø±Ø§Ø¦Ø¹", "Ù…Ù…ØªØ§Ø²", "Ø¬Ù…ÙŠÙ„", "Ø­Ù„Ùˆ", "ğŸ˜Š", "ğŸ˜„", "ğŸ˜ƒ"),
            Emotion.SAD to listOf("Ø­Ø²ÙŠÙ†", "Ø²Ø¹Ù„Ø§Ù†", "Ù…ÙƒØªØ¦Ø¨", "ØªØ¹Ø¨Ø§Ù†", "Ù…ØªØ¶Ø§ÙŠÙ‚", "ğŸ˜¢", "ğŸ˜­", "â˜¹ï¸"),
            Emotion.ANGRY to listOf("ØºØ§Ø¶Ø¨", "Ø²Ø¹Ù„Ø§Ù†", "Ù…ØªÙ†Ø±ÙØ²", "Ù…Ø³ØªØ§Ø¡", "ğŸ˜ ", "ğŸ˜¡", "ğŸ¤¬"),
            Emotion.SURPRISED to listOf("Ù…ØªÙØ§Ø¬Ø¦", "Ù…Ù†Ø¯Ù‡Ø´", "Ù…ØµØ¯ÙˆÙ…", "ğŸ˜²", "ğŸ˜®", "ğŸ˜¯"),
            Emotion.EXCITED to listOf("Ù…ØªØ­Ù…Ø³", "Ù…ØªØ´ÙˆÙ‚", "Ù…Ø¨Ø³ÙˆØ·", "ğŸ¤©", "ğŸ˜", "ğŸ¥³"),
            Emotion.CALM to listOf("Ù‡Ø§Ø¯Ø¦", "Ù…Ø±ØªØ§Ø­", "Ù…Ø·Ù…Ø¦Ù†", "ğŸ˜Œ", "ğŸ˜Š"),
            Emotion.CONFUSED to listOf("Ù…Ø­ØªØ§Ø±", "Ù…Ø´ ÙØ§Ù‡Ù…", "Ù…Ø±ØªØ¨Ùƒ", "ğŸ˜•", "ğŸ¤”", "ğŸ˜µ")
        )
        
        val scores = mutableMapOf<Emotion, Float>()
        
        // Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ù„ÙƒÙ„ Ù…Ø´Ø§Ø¹Ø±
        for ((emotion, keywords) in emotionKeywords) {
            var score = 0f
            for (keyword in keywords) {
                if (lowerText.contains(keyword)) {
                    score += 1f
                }
            }
            scores[emotion] = score / keywords.size
        }
        
        // Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø§Ø· Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø§ÙŠØ¯Ø©
        scores[Emotion.NEUTRAL] = 0.3f
        
        // Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø·
        val primaryEmotion = scores.maxByOrNull { it.value }?.key ?: Emotion.NEUTRAL
        val confidence = scores[primaryEmotion] ?: 0.5f
        
        // Ø­Ø³Ø§Ø¨ Arousal Ùˆ Valence
        val arousal = when (primaryEmotion) {
            Emotion.ANGRY, Emotion.EXCITED, Emotion.SURPRISED -> 0.8f
            Emotion.HAPPY, Emotion.FEAR -> 0.6f
            Emotion.SAD, Emotion.DISGUST -> 0.4f
            else -> 0.3f
        }
        
        val valence = when (primaryEmotion) {
            Emotion.HAPPY, Emotion.EXCITED, Emotion.CALM -> 0.8f
            Emotion.SURPRISED -> 0.6f
            Emotion.NEUTRAL, Emotion.CONFUSED -> 0.5f
            Emotion.SAD, Emotion.FEAR -> 0.3f
            Emotion.ANGRY, Emotion.DISGUST -> 0.2f
        }
        
        return EmotionResult(
            primaryEmotion, confidence, scores.toMap(),
            arousal, valence, "local_text"
        )
    }
    
    /**
     * Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
     */
    private fun extractAudioFeatures(audioData: ByteArray): AudioFeatures {
        // ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© Ù…Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
        val samples = ByteBuffer.wrap(audioData)
            .order(ByteOrder.LITTLE_ENDIAN)
            .asShortBuffer()
            .array()
            .map { it.toFloat() / Short.MAX_VALUE }
        
        // Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        val energy = samples.map { it * it }.average().toFloat()
        val zeroCrossingRate = calculateZeroCrossingRate(samples)
        val pitch = estimatePitch(samples)
        val spectralCentroid = calculateSpectralCentroid(samples)
        
        return AudioFeatures(energy, zeroCrossingRate, pitch, spectralCentroid)
    }
    
    /**
     * ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª Ù…Ø­Ù„ÙŠØ§Ù‹
     */
    private fun analyzeAudioLocally(features: AudioFeatures): EmotionResult {
        val scores = mutableMapOf<Emotion, Float>()
        
        // Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
        
        // Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© = Ø¥Ø«Ø§Ø±Ø© Ø£Ùˆ ØºØ¶Ø¨
        if (features.energy > 0.7f) {
            scores[Emotion.ANGRY] = 0.7f
            scores[Emotion.EXCITED] = 0.6f
        }
        
        // Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© = Ø­Ø²Ù† Ø£Ùˆ Ù‡Ø¯ÙˆØ¡
        if (features.energy < 0.3f) {
            scores[Emotion.SAD] = 0.6f
            scores[Emotion.CALM] = 0.5f
        }
        
        // Ø§Ù„Ù†ØºÙ…Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© = Ø³Ø¹Ø§Ø¯Ø© Ø£Ùˆ ØªÙØ§Ø¬Ø¤
        if (features.pitch > 200f) {
            scores[Emotion.HAPPY] = 0.6f
            scores[Emotion.SURPRISED] = 0.5f
        }
        
        // Ø§Ù„Ù†ØºÙ…Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© = Ø­Ø²Ù† Ø£Ùˆ ØºØ¶Ø¨
        if (features.pitch < 100f) {
            scores[Emotion.SAD] = 0.5f
            scores[Emotion.ANGRY] = 0.4f
        }
        
        // Ù…Ø¹Ø¯Ù„ Ø¹Ø¨ÙˆØ± Ø§Ù„ØµÙØ± Ø§Ù„Ø¹Ø§Ù„ÙŠ = ØªÙˆØªØ±
        if (features.zeroCrossingRate > 0.1f) {
            scores[Emotion.FEAR] = 0.5f
            scores[Emotion.CONFUSED] = 0.4f
        }
        
        scores[Emotion.NEUTRAL] = 0.4f
        
        val primaryEmotion = scores.maxByOrNull { it.value }?.key ?: Emotion.NEUTRAL
        val confidence = scores[primaryEmotion] ?: 0.5f
        
        val arousal = features.energy
        val valence = if (features.pitch > 150f) 0.7f else 0.4f
        
        return EmotionResult(
            primaryEmotion, confidence, scores.toMap(),
            arousal, valence, "local_audio"
        )
    }
    
    /**
     * ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©
     */
    private suspend fun analyzeTextWithCloud(text: String): EmotionResult? {
        // Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬ Google Cloud Natural Language API Ø£Ùˆ Azure Text Analytics
        // Ù„Ù„Ø¨Ø³Ø§Ø·Ø©ØŒ Ø³Ù†Ø¹ÙŠØ¯ null Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø¹Ø¯Ù… Ø§Ù„ØªÙˆÙØ±
        return null
    }
    
    /**
     * ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©
     */
    private suspend fun analyzeAudioWithCloud(audioData: ByteArray): EmotionResult? {
        // Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬ Azure Speech Services Ø£Ùˆ Google Cloud Speech API
        // Ù„Ù„Ø¨Ø³Ø§Ø·Ø©ØŒ Ø³Ù†Ø¹ÙŠØ¯ null Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø¹Ø¯Ù… Ø§Ù„ØªÙˆÙØ±
        return null
    }
    
    // ÙØ¦Ø§Øª Ù…Ø³Ø§Ø¹Ø¯Ø©
    data class AudioFeatures(
        val energy: Float,
        val zeroCrossingRate: Float,
        val pitch: Float,
        val spectralCentroid: Float
    )
    
    // Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
    private fun calculateZeroCrossingRate(samples: List<Float>): Float {
        var crossings = 0
        for (i in 1 until samples.size) {
            if ((samples[i] >= 0) != (samples[i-1] >= 0)) {
                crossings++
            }
        }
        return crossings.toFloat() / samples.size
    }
    
    private fun estimatePitch(samples: List<Float>): Float {
        // ØªÙ‚Ø¯ÙŠØ± Ø¨Ø³ÙŠØ· Ù„Ù„Ù†ØºÙ…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… autocorrelation
        val minPeriod = SAMPLE_RATE / 500 // 500 Hz max
        val maxPeriod = SAMPLE_RATE / 50  // 50 Hz min
        
        var bestPeriod = minPeriod
        var maxCorrelation = 0f
        
        for (period in minPeriod..maxPeriod) {
            var correlation = 0f
            val limit = samples.size - period
            
            for (i in 0 until limit) {
                correlation += samples[i] * samples[i + period]
            }
            
            if (correlation > maxCorrelation) {
                maxCorrelation = correlation
                bestPeriod = period
            }
        }
        
        return SAMPLE_RATE.toFloat() / bestPeriod
    }
    
    private fun calculateSpectralCentroid(samples: List<Float>): Float {
        // Ø­Ø³Ø§Ø¨ Ù…Ø¨Ø³Ø· Ù„Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø·ÙŠÙÙŠ
        val fftSize = 512
        val windowedSamples = samples.take(fftSize)
        
        // ØªØ·Ø¨ÙŠÙ‚ Ù†Ø§ÙØ°Ø© Hamming
        val windowed = windowedSamples.mapIndexed { i, sample ->
            sample * (0.54 - 0.46 * kotlin.math.cos(2 * kotlin.math.PI * i / (fftSize - 1))).toFloat()
        }
        
        // Ø­Ø³Ø§Ø¨ Ù…Ø¨Ø³Ø· Ù„Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø·ÙŠÙÙŠ
        var weightedSum = 0f
        var magnitudeSum = 0f
        
        for (i in windowed.indices) {
            val magnitude = kotlin.math.abs(windowed[i])
            weightedSum += i * magnitude
            magnitudeSum += magnitude
        }
        
        return if (magnitudeSum > 0) weightedSum / magnitudeSum else 0f
    }
    
    /**
     * ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
     */
    fun cleanup() {
        stopRealTimeEmotionAnalysis()
        coroutineScope.cancel()
    }
}

